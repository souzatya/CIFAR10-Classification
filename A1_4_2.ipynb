{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd04cc22-9b2f-4912-b92c-fe4f3bd7b05c",
   "metadata": {},
   "source": [
    "CIFAR10 on AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2484d575-1f5f-4282-bf43-7dba1feeb9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Ep: 1, S:   100] Loss: 2.302\n",
      "[Ep: 1, S:   200] Loss: 2.276\n",
      "[Ep: 1, S:   300] Loss: 2.084\n",
      "[Ep: 1, S:   400] Loss: 1.965\n",
      "[Ep: 1, S:   500] Loss: 1.825\n",
      "[Ep: 1, S:   600] Loss: 1.705\n",
      "[Ep: 1, S:   700] Loss: 1.609\n",
      "[Ep: 2, S:   100] Loss: 1.470\n",
      "[Ep: 2, S:   200] Loss: 1.409\n",
      "[Ep: 2, S:   300] Loss: 1.357\n",
      "[Ep: 2, S:   400] Loss: 1.291\n",
      "[Ep: 2, S:   500] Loss: 1.269\n",
      "[Ep: 2, S:   600] Loss: 1.241\n",
      "[Ep: 2, S:   700] Loss: 1.186\n",
      "[Ep: 3, S:   100] Loss: 1.068\n",
      "[Ep: 3, S:   200] Loss: 1.043\n",
      "[Ep: 3, S:   300] Loss: 1.021\n",
      "[Ep: 3, S:   400] Loss: 0.989\n",
      "[Ep: 3, S:   500] Loss: 0.956\n",
      "[Ep: 3, S:   600] Loss: 0.924\n",
      "[Ep: 3, S:   700] Loss: 0.909\n",
      "[Ep: 4, S:   100] Loss: 0.861\n",
      "[Ep: 4, S:   200] Loss: 0.852\n",
      "[Ep: 4, S:   300] Loss: 0.834\n",
      "[Ep: 4, S:   400] Loss: 0.800\n",
      "[Ep: 4, S:   500] Loss: 0.793\n",
      "[Ep: 4, S:   600] Loss: 0.801\n",
      "[Ep: 4, S:   700] Loss: 0.748\n",
      "[Ep: 5, S:   100] Loss: 0.672\n",
      "[Ep: 5, S:   200] Loss: 0.677\n",
      "[Ep: 5, S:   300] Loss: 0.710\n",
      "[Ep: 5, S:   400] Loss: 0.682\n",
      "[Ep: 5, S:   500] Loss: 0.658\n",
      "[Ep: 5, S:   600] Loss: 0.674\n",
      "[Ep: 5, S:   700] Loss: 0.659\n",
      "[Ep: 6, S:   100] Loss: 0.577\n",
      "[Ep: 6, S:   200] Loss: 0.591\n",
      "[Ep: 6, S:   300] Loss: 0.580\n",
      "[Ep: 6, S:   400] Loss: 0.574\n",
      "[Ep: 6, S:   500] Loss: 0.576\n",
      "[Ep: 6, S:   600] Loss: 0.591\n",
      "[Ep: 6, S:   700] Loss: 0.616\n",
      "[Ep: 7, S:   100] Loss: 0.497\n",
      "[Ep: 7, S:   200] Loss: 0.499\n",
      "[Ep: 7, S:   300] Loss: 0.504\n",
      "[Ep: 7, S:   400] Loss: 0.498\n",
      "[Ep: 7, S:   500] Loss: 0.522\n",
      "[Ep: 7, S:   600] Loss: 0.483\n",
      "[Ep: 7, S:   700] Loss: 0.504\n",
      "[Ep: 8, S:   100] Loss: 0.402\n",
      "[Ep: 8, S:   200] Loss: 0.423\n",
      "[Ep: 8, S:   300] Loss: 0.431\n",
      "[Ep: 8, S:   400] Loss: 0.459\n",
      "[Ep: 8, S:   500] Loss: 0.447\n",
      "[Ep: 8, S:   600] Loss: 0.455\n",
      "[Ep: 8, S:   700] Loss: 0.448\n",
      "[Ep: 9, S:   100] Loss: 0.346\n",
      "[Ep: 9, S:   200] Loss: 0.359\n",
      "[Ep: 9, S:   300] Loss: 0.376\n",
      "[Ep: 9, S:   400] Loss: 0.384\n",
      "[Ep: 9, S:   500] Loss: 0.374\n",
      "[Ep: 9, S:   600] Loss: 0.394\n",
      "[Ep: 9, S:   700] Loss: 0.391\n",
      "[Ep: 10, S:   100] Loss: 0.319\n",
      "[Ep: 10, S:   200] Loss: 0.343\n",
      "[Ep: 10, S:   300] Loss: 0.335\n",
      "[Ep: 10, S:   400] Loss: 0.319\n",
      "[Ep: 10, S:   500] Loss: 0.339\n",
      "[Ep: 10, S:   600] Loss: 0.349\n",
      "[Ep: 10, S:   700] Loss: 0.378\n",
      "Finished Training\n",
      "Accuracy: 79.87 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as op\n",
    "\n",
    "# Set GPU\n",
    "dev = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dataset transformations\n",
    "tf = trans.Compose([\n",
    "    trans.Resize((224, 224)),  # AlexNet expects 224x224 input\n",
    "    trans.ToTensor(),\n",
    "    trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Train\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tf)\n",
    "train_l = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR10 Test\n",
    "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tf)\n",
    "test_l = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "#Load AlexNet\n",
    "Alexnet = models.alexnet()\n",
    "\n",
    "# Modified last FC layer to fit CIFAR10\n",
    "n_ft = Alexnet.classifier[6].in_features\n",
    "Alexnet.classifier[6] = nn.Linear(n_ft, 10)\n",
    "\n",
    "# To GPU\n",
    "Alexnet.to(dev)\n",
    "\n",
    "# Loss function and optimizer\n",
    "c = nn.CrossEntropyLoss()\n",
    "o = op.SGD(Alexnet.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "num_ep = 10\n",
    "for ep in range(num_ep):\n",
    "    run_loss = 0.0\n",
    "    for i, data in enumerate(train_l, 0):\n",
    "        inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "        o.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = Alexnet(inputs)\n",
    "        loss = c(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        o.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Ep: %d, S: %5d] Loss: %.3f' %\n",
    "                  (ep + 1, i + 1, run_loss / 100))\n",
    "            run_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(ntw, dloader):\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dloader:\n",
    "            inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "            outputs = ntw(inputs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            corr += (pred == labels).sum().item()\n",
    "    return corr / total\n",
    "\n",
    "# Test\n",
    "acc = accuracy(Alexnet, test_l)\n",
    "print('Accuracy: %.2f %%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d88b8f-0033-474c-842f-6aa63046c922",
   "metadata": {},
   "source": [
    "CIFAR10 on VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "157b3249-0e47-4337-b966-ba440bd04719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Ep: 1, S:   100] Loss: 2.254\n",
      "[Ep: 1, S:   200] Loss: 2.065\n",
      "[Ep: 1, S:   300] Loss: 1.939\n",
      "[Ep: 1, S:   400] Loss: 1.821\n",
      "[Ep: 1, S:   500] Loss: 1.720\n",
      "[Ep: 1, S:   600] Loss: 1.615\n",
      "[Ep: 1, S:   700] Loss: 1.542\n",
      "[Ep: 2, S:   100] Loss: 1.425\n",
      "[Ep: 2, S:   200] Loss: 1.362\n",
      "[Ep: 2, S:   300] Loss: 1.302\n",
      "[Ep: 2, S:   400] Loss: 1.262\n",
      "[Ep: 2, S:   500] Loss: 1.196\n",
      "[Ep: 2, S:   600] Loss: 1.180\n",
      "[Ep: 2, S:   700] Loss: 1.120\n",
      "[Ep: 3, S:   100] Loss: 1.022\n",
      "[Ep: 3, S:   200] Loss: 0.953\n",
      "[Ep: 3, S:   300] Loss: 0.942\n",
      "[Ep: 3, S:   400] Loss: 0.909\n",
      "[Ep: 3, S:   500] Loss: 0.907\n",
      "[Ep: 3, S:   600] Loss: 0.868\n",
      "[Ep: 3, S:   700] Loss: 0.844\n",
      "[Ep: 4, S:   100] Loss: 0.752\n",
      "[Ep: 4, S:   200] Loss: 0.714\n",
      "[Ep: 4, S:   300] Loss: 0.739\n",
      "[Ep: 4, S:   400] Loss: 0.722\n",
      "[Ep: 4, S:   500] Loss: 0.704\n",
      "[Ep: 4, S:   600] Loss: 0.682\n",
      "[Ep: 4, S:   700] Loss: 0.694\n",
      "[Ep: 5, S:   100] Loss: 0.553\n",
      "[Ep: 5, S:   200] Loss: 0.521\n",
      "[Ep: 5, S:   300] Loss: 0.579\n",
      "[Ep: 5, S:   400] Loss: 0.559\n",
      "[Ep: 5, S:   500] Loss: 0.582\n",
      "[Ep: 5, S:   600] Loss: 0.565\n",
      "[Ep: 5, S:   700] Loss: 0.548\n",
      "[Ep: 6, S:   100] Loss: 0.409\n",
      "[Ep: 6, S:   200] Loss: 0.417\n",
      "[Ep: 6, S:   300] Loss: 0.414\n",
      "[Ep: 6, S:   400] Loss: 0.438\n",
      "[Ep: 6, S:   500] Loss: 0.418\n",
      "[Ep: 6, S:   600] Loss: 0.458\n",
      "[Ep: 6, S:   700] Loss: 0.430\n",
      "[Ep: 7, S:   100] Loss: 0.267\n",
      "[Ep: 7, S:   200] Loss: 0.301\n",
      "[Ep: 7, S:   300] Loss: 0.341\n",
      "[Ep: 7, S:   400] Loss: 0.314\n",
      "[Ep: 7, S:   500] Loss: 0.343\n",
      "[Ep: 7, S:   600] Loss: 0.326\n",
      "[Ep: 7, S:   700] Loss: 0.340\n",
      "[Ep: 8, S:   100] Loss: 0.212\n",
      "[Ep: 8, S:   200] Loss: 0.211\n",
      "[Ep: 8, S:   300] Loss: 0.241\n",
      "[Ep: 8, S:   400] Loss: 0.224\n",
      "[Ep: 8, S:   500] Loss: 0.236\n",
      "[Ep: 8, S:   600] Loss: 0.275\n",
      "[Ep: 8, S:   700] Loss: 0.245\n",
      "[Ep: 9, S:   100] Loss: 0.136\n",
      "[Ep: 9, S:   200] Loss: 0.164\n",
      "[Ep: 9, S:   300] Loss: 0.150\n",
      "[Ep: 9, S:   400] Loss: 0.184\n",
      "[Ep: 9, S:   500] Loss: 0.166\n",
      "[Ep: 9, S:   600] Loss: 0.181\n",
      "[Ep: 9, S:   700] Loss: 0.196\n",
      "[Ep: 10, S:   100] Loss: 0.113\n",
      "[Ep: 10, S:   200] Loss: 0.132\n",
      "[Ep: 10, S:   300] Loss: 0.135\n",
      "[Ep: 10, S:   400] Loss: 0.130\n",
      "[Ep: 10, S:   500] Loss: 0.146\n",
      "[Ep: 10, S:   600] Loss: 0.144\n",
      "[Ep: 10, S:   700] Loss: 0.159\n",
      "Finished Training\n",
      "Accuracy: 77.87 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as op\n",
    "\n",
    "# Set GPU\n",
    "dev = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dataset transformations\n",
    "tf = trans.Compose([\n",
    "    trans.Resize((224, 224)),  # AlexNet expects 224x224 input\n",
    "    trans.ToTensor(),\n",
    "    trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Train\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tf)\n",
    "train_l = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR10 Test\n",
    "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tf)\n",
    "test_l = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "#Load VGG-16\n",
    "Vgg = models.vgg16(num_classes=10)\n",
    "\n",
    "# To GPU\n",
    "Vgg.to(dev)\n",
    "\n",
    "# Loss function and optimizer\n",
    "c = nn.CrossEntropyLoss()\n",
    "o = op.SGD(Vgg.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "num_ep = 10\n",
    "for ep in range(num_ep):\n",
    "    run_loss = 0.0\n",
    "    for i, data in enumerate(train_l, 0):\n",
    "        inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "        o.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = Vgg(inputs)\n",
    "        loss = c(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        o.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Ep: %d, S: %5d] Loss: %.3f' %\n",
    "                  (ep + 1, i + 1, run_loss / 100))\n",
    "            run_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(ntw, dloader):\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dloader:\n",
    "            inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "            outputs = ntw(inputs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            corr += (pred == labels).sum().item()\n",
    "    return corr / total\n",
    "\n",
    "# Test\n",
    "acc = accuracy(Vgg, test_l)\n",
    "print('Accuracy: %.2f %%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7440ea3-b211-4d63-85d9-66dd8c251889",
   "metadata": {},
   "source": [
    "CIFAR10 on GoogleNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "684b1d6e-f8b2-4e32-af05-edaf2b90b073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Ep: 1, S:   100] Loss: 1.951\n",
      "[Ep: 1, S:   200] Loss: 1.737\n",
      "[Ep: 1, S:   300] Loss: 1.647\n",
      "[Ep: 1, S:   400] Loss: 1.531\n",
      "[Ep: 1, S:   500] Loss: 1.493\n",
      "[Ep: 1, S:   600] Loss: 1.406\n",
      "[Ep: 1, S:   700] Loss: 1.343\n",
      "[Ep: 2, S:   100] Loss: 1.205\n",
      "[Ep: 2, S:   200] Loss: 1.133\n",
      "[Ep: 2, S:   300] Loss: 1.059\n",
      "[Ep: 2, S:   400] Loss: 1.043\n",
      "[Ep: 2, S:   500] Loss: 0.989\n",
      "[Ep: 2, S:   600] Loss: 0.951\n",
      "[Ep: 2, S:   700] Loss: 0.904\n",
      "[Ep: 3, S:   100] Loss: 0.829\n",
      "[Ep: 3, S:   200] Loss: 0.819\n",
      "[Ep: 3, S:   300] Loss: 0.803\n",
      "[Ep: 3, S:   400] Loss: 0.774\n",
      "[Ep: 3, S:   500] Loss: 0.773\n",
      "[Ep: 3, S:   600] Loss: 0.765\n",
      "[Ep: 3, S:   700] Loss: 0.704\n",
      "[Ep: 4, S:   100] Loss: 0.649\n",
      "[Ep: 4, S:   200] Loss: 0.628\n",
      "[Ep: 4, S:   300] Loss: 0.615\n",
      "[Ep: 4, S:   400] Loss: 0.614\n",
      "[Ep: 4, S:   500] Loss: 0.612\n",
      "[Ep: 4, S:   600] Loss: 0.630\n",
      "[Ep: 4, S:   700] Loss: 0.568\n",
      "[Ep: 5, S:   100] Loss: 0.492\n",
      "[Ep: 5, S:   200] Loss: 0.533\n",
      "[Ep: 5, S:   300] Loss: 0.496\n",
      "[Ep: 5, S:   400] Loss: 0.518\n",
      "[Ep: 5, S:   500] Loss: 0.489\n",
      "[Ep: 5, S:   600] Loss: 0.492\n",
      "[Ep: 5, S:   700] Loss: 0.485\n",
      "[Ep: 6, S:   100] Loss: 0.410\n",
      "[Ep: 6, S:   200] Loss: 0.393\n",
      "[Ep: 6, S:   300] Loss: 0.421\n",
      "[Ep: 6, S:   400] Loss: 0.422\n",
      "[Ep: 6, S:   500] Loss: 0.453\n",
      "[Ep: 6, S:   600] Loss: 0.424\n",
      "[Ep: 6, S:   700] Loss: 0.424\n",
      "[Ep: 7, S:   100] Loss: 0.334\n",
      "[Ep: 7, S:   200] Loss: 0.340\n",
      "[Ep: 7, S:   300] Loss: 0.344\n",
      "[Ep: 7, S:   400] Loss: 0.352\n",
      "[Ep: 7, S:   500] Loss: 0.357\n",
      "[Ep: 7, S:   600] Loss: 0.371\n",
      "[Ep: 7, S:   700] Loss: 0.357\n",
      "[Ep: 8, S:   100] Loss: 0.278\n",
      "[Ep: 8, S:   200] Loss: 0.276\n",
      "[Ep: 8, S:   300] Loss: 0.282\n",
      "[Ep: 8, S:   400] Loss: 0.304\n",
      "[Ep: 8, S:   500] Loss: 0.311\n",
      "[Ep: 8, S:   600] Loss: 0.310\n",
      "[Ep: 8, S:   700] Loss: 0.313\n",
      "[Ep: 9, S:   100] Loss: 0.230\n",
      "[Ep: 9, S:   200] Loss: 0.248\n",
      "[Ep: 9, S:   300] Loss: 0.235\n",
      "[Ep: 9, S:   400] Loss: 0.245\n",
      "[Ep: 9, S:   500] Loss: 0.243\n",
      "[Ep: 9, S:   600] Loss: 0.274\n",
      "[Ep: 9, S:   700] Loss: 0.279\n",
      "[Ep: 10, S:   100] Loss: 0.197\n",
      "[Ep: 10, S:   200] Loss: 0.168\n",
      "[Ep: 10, S:   300] Loss: 0.195\n",
      "[Ep: 10, S:   400] Loss: 0.194\n",
      "[Ep: 10, S:   500] Loss: 0.218\n",
      "[Ep: 10, S:   600] Loss: 0.218\n",
      "[Ep: 10, S:   700] Loss: 0.228\n",
      "Finished Training\n",
      "Accuracy: 83.54 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as op\n",
    "\n",
    "# Set GPU\n",
    "dev = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dataset transformations\n",
    "tf = trans.Compose([\n",
    "    trans.Resize((224, 224)),  # AlexNet expects 224x224 input\n",
    "    trans.ToTensor(),\n",
    "    trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Train\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tf)\n",
    "train_l = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR10 Test\n",
    "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tf)\n",
    "test_l = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "#Load VGG-16\n",
    "Googlenet = models.googlenet(init_weights=True)\n",
    "\n",
    "# Modified last FC layer to fit CIFAR10\n",
    "Googlenet.fc = nn.Linear(1024, 10)\n",
    "\n",
    "\n",
    "# To GPU\n",
    "Googlenet.to(dev)\n",
    "\n",
    "# Loss function and optimizer\n",
    "c = nn.CrossEntropyLoss()\n",
    "o = op.SGD(Googlenet.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "num_ep = 10\n",
    "for ep in range(num_ep):\n",
    "    run_loss = 0.0\n",
    "    for i, data in enumerate(train_l, 0):\n",
    "        inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "        o.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = Googlenet(inputs)[0]\n",
    "        loss = c(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        o.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Ep: %d, S: %5d] Loss: %.3f' %\n",
    "                  (ep + 1, i + 1, run_loss / 100))\n",
    "            run_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(ntw, dloader):\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dloader:\n",
    "            inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "            outputs = ntw(inputs)[0]\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            corr += (pred == labels).sum().item()\n",
    "    return corr / total\n",
    "\n",
    "# Test\n",
    "acc = accuracy(Googlenet, test_l)\n",
    "print('Accuracy: %.2f %%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d69f36-7eb2-4c87-8d91-c9967566e5eb",
   "metadata": {},
   "source": [
    "CIFAR10 on ResNet152 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ada10d-3e69-4181-95b9-e4e04e9412fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Ep: 1, S:   100] Loss: 2.800\n",
      "[Ep: 1, S:   200] Loss: 2.070\n",
      "[Ep: 1, S:   300] Loss: 1.904\n",
      "[Ep: 1, S:   400] Loss: 1.818\n",
      "[Ep: 1, S:   500] Loss: 1.730\n",
      "[Ep: 1, S:   600] Loss: 1.683\n",
      "[Ep: 1, S:   700] Loss: 1.594\n",
      "[Ep: 2, S:   100] Loss: 1.546\n",
      "[Ep: 2, S:   200] Loss: 1.502\n",
      "[Ep: 2, S:   300] Loss: 1.444\n",
      "[Ep: 2, S:   400] Loss: 1.432\n",
      "[Ep: 2, S:   500] Loss: 1.419\n",
      "[Ep: 2, S:   600] Loss: 1.354\n",
      "[Ep: 2, S:   700] Loss: 1.328\n",
      "[Ep: 3, S:   100] Loss: 1.250\n",
      "[Ep: 3, S:   200] Loss: 1.236\n",
      "[Ep: 3, S:   300] Loss: 1.188\n",
      "[Ep: 3, S:   400] Loss: 1.172\n",
      "[Ep: 3, S:   500] Loss: 1.179\n",
      "[Ep: 3, S:   600] Loss: 1.143\n",
      "[Ep: 3, S:   700] Loss: 1.100\n",
      "[Ep: 4, S:   100] Loss: 0.989\n",
      "[Ep: 4, S:   200] Loss: 0.990\n",
      "[Ep: 4, S:   300] Loss: 0.970\n",
      "[Ep: 4, S:   400] Loss: 0.946\n",
      "[Ep: 4, S:   500] Loss: 0.947\n",
      "[Ep: 4, S:   600] Loss: 0.957\n",
      "[Ep: 4, S:   700] Loss: 0.882\n",
      "[Ep: 5, S:   100] Loss: 0.816\n",
      "[Ep: 5, S:   200] Loss: 0.781\n",
      "[Ep: 5, S:   300] Loss: 0.790\n",
      "[Ep: 5, S:   400] Loss: 0.779\n",
      "[Ep: 5, S:   500] Loss: 0.749\n",
      "[Ep: 5, S:   600] Loss: 0.748\n",
      "[Ep: 5, S:   700] Loss: 0.739\n",
      "[Ep: 6, S:   100] Loss: 0.629\n",
      "[Ep: 6, S:   200] Loss: 0.624\n",
      "[Ep: 6, S:   300] Loss: 0.606\n",
      "[Ep: 6, S:   400] Loss: 0.635\n",
      "[Ep: 6, S:   500] Loss: 0.625\n",
      "[Ep: 6, S:   600] Loss: 0.610\n",
      "[Ep: 6, S:   700] Loss: 0.614\n",
      "[Ep: 7, S:   100] Loss: 0.455\n",
      "[Ep: 7, S:   200] Loss: 0.476\n",
      "[Ep: 7, S:   300] Loss: 0.493\n",
      "[Ep: 7, S:   400] Loss: 0.509\n",
      "[Ep: 7, S:   500] Loss: 0.484\n",
      "[Ep: 7, S:   600] Loss: 0.523\n",
      "[Ep: 7, S:   700] Loss: 0.497\n",
      "[Ep: 8, S:   100] Loss: 0.340\n",
      "[Ep: 8, S:   200] Loss: 0.357\n",
      "[Ep: 8, S:   300] Loss: 0.414\n",
      "[Ep: 8, S:   400] Loss: 0.414\n",
      "[Ep: 8, S:   500] Loss: 0.439\n",
      "[Ep: 8, S:   600] Loss: 0.416\n",
      "[Ep: 8, S:   700] Loss: 0.424\n",
      "[Ep: 9, S:   100] Loss: 0.253\n",
      "[Ep: 9, S:   200] Loss: 0.296\n",
      "[Ep: 9, S:   300] Loss: 0.347\n",
      "[Ep: 9, S:   400] Loss: 0.335\n",
      "[Ep: 9, S:   500] Loss: 0.355\n",
      "[Ep: 9, S:   600] Loss: 0.377\n",
      "[Ep: 9, S:   700] Loss: 0.388\n",
      "[Ep: 10, S:   100] Loss: 0.193\n",
      "[Ep: 10, S:   200] Loss: 0.211\n",
      "[Ep: 10, S:   300] Loss: 0.267\n",
      "[Ep: 10, S:   400] Loss: 0.279\n",
      "[Ep: 10, S:   500] Loss: 0.285\n",
      "[Ep: 10, S:   600] Loss: 0.282\n",
      "[Ep: 10, S:   700] Loss: 0.300\n",
      "Finished Training\n",
      "Accuracy: 77.77 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as trans\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as op\n",
    "\n",
    "# Set GPU\n",
    "dev = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dataset transformations\n",
    "tf = trans.Compose([\n",
    "    trans.Resize((224, 224)),  # AlexNet expects 224x224 input\n",
    "    trans.ToTensor(),\n",
    "    trans.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR10 Train\n",
    "train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=tf)\n",
    "train_l = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load CIFAR10 Test\n",
    "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=tf)\n",
    "test_l = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "#Load Resnet152\n",
    "Resnet = models.resnet152()\n",
    "\n",
    "# Modified last FC layer to fit CIFAR10\n",
    "Resnet.fc = nn.Linear(2048, 10)\n",
    "\n",
    "\n",
    "# To GPU\n",
    "Resnet.to(dev)\n",
    "\n",
    "# Loss function and optimizer\n",
    "c = nn.CrossEntropyLoss()\n",
    "o = op.SGD(Resnet.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training\n",
    "num_ep = 10\n",
    "for ep in range(num_ep):\n",
    "    run_loss = 0.0\n",
    "    for i, data in enumerate(train_l, 0):\n",
    "        inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "        o.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = Resnet(inputs)\n",
    "        loss = c(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        o.step()\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Ep: %d, S: %5d] Loss: %.3f' %\n",
    "                  (ep + 1, i + 1, run_loss / 100))\n",
    "            run_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Accuracy\n",
    "def accuracy(ntw, dloader):\n",
    "    corr = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dloader:\n",
    "            inputs, labels = data[0].to(dev), data[1].to(dev)\n",
    "            outputs = ntw(inputs)\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            corr += (pred == labels).sum().item()\n",
    "    return corr / total\n",
    "\n",
    "# Test\n",
    "acc = accuracy(Resnet, test_l)\n",
    "print('Accuracy: %.2f %%' % (100 * acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
