{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-PCsMxZv-B4"
   },
   "source": [
    "## **Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM9LDDunv7N_",
    "outputId": "fc79f7d1-4ffc-48ac-d7ee-d8c32583f652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:13<00:00, 12715387.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch [1/10], Step [100/782], Loss: 1.6014\n",
      "Epoch [1/10], Step [200/782], Loss: 1.7285\n",
      "Epoch [1/10], Step [300/782], Loss: 1.2765\n",
      "Epoch [1/10], Step [400/782], Loss: 1.2570\n",
      "Epoch [1/10], Step [500/782], Loss: 1.2392\n",
      "Epoch [1/10], Step [600/782], Loss: 0.9949\n",
      "Epoch [1/10], Step [700/782], Loss: 1.2892\n",
      "Epoch [2/10], Step [100/782], Loss: 1.2479\n",
      "Epoch [2/10], Step [200/782], Loss: 1.0150\n",
      "Epoch [2/10], Step [300/782], Loss: 1.0945\n",
      "Epoch [2/10], Step [400/782], Loss: 0.8884\n",
      "Epoch [2/10], Step [500/782], Loss: 0.7958\n",
      "Epoch [2/10], Step [600/782], Loss: 1.0781\n",
      "Epoch [2/10], Step [700/782], Loss: 0.9117\n",
      "Epoch [3/10], Step [100/782], Loss: 1.1361\n",
      "Epoch [3/10], Step [200/782], Loss: 0.9273\n",
      "Epoch [3/10], Step [300/782], Loss: 0.8810\n",
      "Epoch [3/10], Step [400/782], Loss: 0.9124\n",
      "Epoch [3/10], Step [500/782], Loss: 0.7568\n",
      "Epoch [3/10], Step [600/782], Loss: 0.9125\n",
      "Epoch [3/10], Step [700/782], Loss: 0.8653\n",
      "Epoch [4/10], Step [100/782], Loss: 0.7774\n",
      "Epoch [4/10], Step [200/782], Loss: 0.7719\n",
      "Epoch [4/10], Step [300/782], Loss: 0.7761\n",
      "Epoch [4/10], Step [400/782], Loss: 0.5034\n",
      "Epoch [4/10], Step [500/782], Loss: 0.6923\n",
      "Epoch [4/10], Step [600/782], Loss: 0.9134\n",
      "Epoch [4/10], Step [700/782], Loss: 0.7542\n",
      "Epoch [5/10], Step [100/782], Loss: 0.6674\n",
      "Epoch [5/10], Step [200/782], Loss: 0.8065\n",
      "Epoch [5/10], Step [300/782], Loss: 0.6523\n",
      "Epoch [5/10], Step [400/782], Loss: 0.5765\n",
      "Epoch [5/10], Step [500/782], Loss: 0.7250\n",
      "Epoch [5/10], Step [600/782], Loss: 0.6852\n",
      "Epoch [5/10], Step [700/782], Loss: 0.5991\n",
      "Epoch [6/10], Step [100/782], Loss: 0.6234\n",
      "Epoch [6/10], Step [200/782], Loss: 0.6914\n",
      "Epoch [6/10], Step [300/782], Loss: 0.6816\n",
      "Epoch [6/10], Step [400/782], Loss: 0.5768\n",
      "Epoch [6/10], Step [500/782], Loss: 0.6364\n",
      "Epoch [6/10], Step [600/782], Loss: 0.4679\n",
      "Epoch [6/10], Step [700/782], Loss: 0.5848\n",
      "Epoch [7/10], Step [100/782], Loss: 0.4693\n",
      "Epoch [7/10], Step [200/782], Loss: 0.5736\n",
      "Epoch [7/10], Step [300/782], Loss: 0.4834\n",
      "Epoch [7/10], Step [400/782], Loss: 0.5990\n",
      "Epoch [7/10], Step [500/782], Loss: 0.4021\n",
      "Epoch [7/10], Step [600/782], Loss: 0.5518\n",
      "Epoch [7/10], Step [700/782], Loss: 0.3942\n",
      "Epoch [8/10], Step [100/782], Loss: 0.4133\n",
      "Epoch [8/10], Step [200/782], Loss: 0.3664\n",
      "Epoch [8/10], Step [300/782], Loss: 0.2451\n",
      "Epoch [8/10], Step [400/782], Loss: 0.4930\n",
      "Epoch [8/10], Step [500/782], Loss: 0.3893\n",
      "Epoch [8/10], Step [600/782], Loss: 0.5398\n",
      "Epoch [8/10], Step [700/782], Loss: 0.5041\n",
      "Epoch [9/10], Step [100/782], Loss: 0.4044\n",
      "Epoch [9/10], Step [200/782], Loss: 0.3716\n",
      "Epoch [9/10], Step [300/782], Loss: 0.4640\n",
      "Epoch [9/10], Step [400/782], Loss: 0.4240\n",
      "Epoch [9/10], Step [500/782], Loss: 0.4071\n",
      "Epoch [9/10], Step [600/782], Loss: 0.6575\n",
      "Epoch [9/10], Step [700/782], Loss: 0.5266\n",
      "Epoch [10/10], Step [100/782], Loss: 0.2383\n",
      "Epoch [10/10], Step [200/782], Loss: 0.2672\n",
      "Epoch [10/10], Step [300/782], Loss: 0.6874\n",
      "Epoch [10/10], Step [400/782], Loss: 0.2830\n",
      "Epoch [10/10], Step [500/782], Loss: 0.3002\n",
      "Epoch [10/10], Step [600/782], Loss: 0.5157\n",
      "Epoch [10/10], Step [700/782], Loss: 0.3185\n",
      "Test Accuracy of the model on the 10000 test images: 73.94 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Convolutional neural network (3 Convolutional layers + 1 Fully connected layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOcj2sVqIdof",
    "outputId": "db65f432-f3e9-4a5c-ec13-42d8d821b68e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/100], Step [100/782], Loss: 1.9820\n",
      "Epoch [1/100], Step [200/782], Loss: 1.7135\n",
      "Epoch [1/100], Step [300/782], Loss: 1.6176\n",
      "Epoch [1/100], Step [400/782], Loss: 1.4844\n",
      "Epoch [1/100], Step [500/782], Loss: 1.6111\n",
      "Epoch [1/100], Step [600/782], Loss: 1.6146\n",
      "Epoch [1/100], Step [700/782], Loss: 1.3368\n",
      "Epoch [2/100], Step [100/782], Loss: 1.5575\n",
      "Epoch [2/100], Step [200/782], Loss: 1.3339\n",
      "Epoch [2/100], Step [300/782], Loss: 1.4902\n",
      "Epoch [2/100], Step [400/782], Loss: 1.2101\n",
      "Epoch [2/100], Step [500/782], Loss: 1.1109\n",
      "Epoch [2/100], Step [600/782], Loss: 0.9728\n",
      "Epoch [2/100], Step [700/782], Loss: 1.0572\n",
      "Epoch [3/100], Step [100/782], Loss: 1.3173\n",
      "Epoch [3/100], Step [200/782], Loss: 1.2741\n",
      "Epoch [3/100], Step [300/782], Loss: 1.4302\n",
      "Epoch [3/100], Step [400/782], Loss: 1.2798\n",
      "Epoch [3/100], Step [500/782], Loss: 1.4131\n",
      "Epoch [3/100], Step [600/782], Loss: 1.2670\n",
      "Epoch [3/100], Step [700/782], Loss: 1.1009\n",
      "Epoch [4/100], Step [100/782], Loss: 1.1808\n",
      "Epoch [4/100], Step [200/782], Loss: 1.0415\n",
      "Epoch [4/100], Step [300/782], Loss: 1.2919\n",
      "Epoch [4/100], Step [400/782], Loss: 1.1713\n",
      "Epoch [4/100], Step [500/782], Loss: 1.0211\n",
      "Epoch [4/100], Step [600/782], Loss: 0.9457\n",
      "Epoch [4/100], Step [700/782], Loss: 1.1088\n",
      "Epoch [5/100], Step [100/782], Loss: 1.0276\n",
      "Epoch [5/100], Step [200/782], Loss: 1.0364\n",
      "Epoch [5/100], Step [300/782], Loss: 1.1126\n",
      "Epoch [5/100], Step [400/782], Loss: 1.0716\n",
      "Epoch [5/100], Step [500/782], Loss: 1.0382\n",
      "Epoch [5/100], Step [600/782], Loss: 1.0881\n",
      "Epoch [5/100], Step [700/782], Loss: 1.0051\n",
      "Epoch [6/100], Step [100/782], Loss: 0.8950\n",
      "Epoch [6/100], Step [200/782], Loss: 0.9201\n",
      "Epoch [6/100], Step [300/782], Loss: 0.9283\n",
      "Epoch [6/100], Step [400/782], Loss: 0.8832\n",
      "Epoch [6/100], Step [500/782], Loss: 0.8411\n",
      "Epoch [6/100], Step [600/782], Loss: 0.9008\n",
      "Epoch [6/100], Step [700/782], Loss: 1.0167\n",
      "Epoch [7/100], Step [100/782], Loss: 0.8144\n",
      "Epoch [7/100], Step [200/782], Loss: 0.8912\n",
      "Epoch [7/100], Step [300/782], Loss: 1.0213\n",
      "Epoch [7/100], Step [400/782], Loss: 0.8483\n",
      "Epoch [7/100], Step [500/782], Loss: 0.7289\n",
      "Epoch [7/100], Step [600/782], Loss: 0.9009\n",
      "Epoch [7/100], Step [700/782], Loss: 0.7400\n",
      "Epoch [8/100], Step [100/782], Loss: 0.7165\n",
      "Epoch [8/100], Step [200/782], Loss: 0.9253\n",
      "Epoch [8/100], Step [300/782], Loss: 0.8996\n",
      "Epoch [8/100], Step [400/782], Loss: 0.8354\n",
      "Epoch [8/100], Step [500/782], Loss: 0.7923\n",
      "Epoch [8/100], Step [600/782], Loss: 0.9129\n",
      "Epoch [8/100], Step [700/782], Loss: 1.0218\n",
      "Epoch [9/100], Step [100/782], Loss: 0.9692\n",
      "Epoch [9/100], Step [200/782], Loss: 0.8451\n",
      "Epoch [9/100], Step [300/782], Loss: 0.7376\n",
      "Epoch [9/100], Step [400/782], Loss: 0.7111\n",
      "Epoch [9/100], Step [500/782], Loss: 0.6798\n",
      "Epoch [9/100], Step [600/782], Loss: 0.9408\n",
      "Epoch [9/100], Step [700/782], Loss: 0.8922\n",
      "Epoch [10/100], Step [100/782], Loss: 0.7035\n",
      "Epoch [10/100], Step [200/782], Loss: 0.6602\n",
      "Epoch [10/100], Step [300/782], Loss: 0.8417\n",
      "Epoch [10/100], Step [400/782], Loss: 0.9840\n",
      "Epoch [10/100], Step [500/782], Loss: 0.5727\n",
      "Epoch [10/100], Step [600/782], Loss: 0.7210\n",
      "Epoch [10/100], Step [700/782], Loss: 0.6983\n",
      "Epoch [11/100], Step [100/782], Loss: 0.7510\n",
      "Epoch [11/100], Step [200/782], Loss: 0.8897\n",
      "Epoch [11/100], Step [300/782], Loss: 0.8210\n",
      "Epoch [11/100], Step [400/782], Loss: 0.7187\n",
      "Epoch [11/100], Step [500/782], Loss: 0.8820\n",
      "Epoch [11/100], Step [600/782], Loss: 0.7166\n",
      "Epoch [11/100], Step [700/782], Loss: 0.7315\n",
      "Epoch [12/100], Step [100/782], Loss: 0.7706\n",
      "Epoch [12/100], Step [200/782], Loss: 0.9119\n",
      "Epoch [12/100], Step [300/782], Loss: 0.8220\n",
      "Epoch [12/100], Step [400/782], Loss: 0.8740\n",
      "Epoch [12/100], Step [500/782], Loss: 0.8691\n",
      "Epoch [12/100], Step [600/782], Loss: 0.8021\n",
      "Epoch [12/100], Step [700/782], Loss: 0.7710\n",
      "Epoch [13/100], Step [100/782], Loss: 0.7688\n",
      "Epoch [13/100], Step [200/782], Loss: 0.8747\n",
      "Epoch [13/100], Step [300/782], Loss: 0.7091\n",
      "Epoch [13/100], Step [400/782], Loss: 0.9241\n",
      "Epoch [13/100], Step [500/782], Loss: 0.6018\n",
      "Epoch [13/100], Step [600/782], Loss: 0.7249\n",
      "Epoch [13/100], Step [700/782], Loss: 0.7430\n",
      "Epoch [14/100], Step [100/782], Loss: 0.7585\n",
      "Epoch [14/100], Step [200/782], Loss: 0.5901\n",
      "Epoch [14/100], Step [300/782], Loss: 0.6974\n",
      "Epoch [14/100], Step [400/782], Loss: 0.7836\n",
      "Epoch [14/100], Step [500/782], Loss: 0.8946\n",
      "Epoch [14/100], Step [600/782], Loss: 0.8931\n",
      "Epoch [14/100], Step [700/782], Loss: 0.9654\n",
      "Epoch [15/100], Step [100/782], Loss: 0.9905\n",
      "Epoch [15/100], Step [200/782], Loss: 0.8753\n",
      "Epoch [15/100], Step [300/782], Loss: 0.6686\n",
      "Epoch [15/100], Step [400/782], Loss: 0.7310\n",
      "Epoch [15/100], Step [500/782], Loss: 0.6135\n",
      "Epoch [15/100], Step [600/782], Loss: 0.5091\n",
      "Epoch [15/100], Step [700/782], Loss: 0.6115\n",
      "Epoch [16/100], Step [100/782], Loss: 0.6715\n",
      "Epoch [16/100], Step [200/782], Loss: 0.9117\n",
      "Epoch [16/100], Step [300/782], Loss: 0.7698\n",
      "Epoch [16/100], Step [400/782], Loss: 0.6805\n",
      "Epoch [16/100], Step [500/782], Loss: 0.5869\n",
      "Epoch [16/100], Step [600/782], Loss: 0.5877\n",
      "Epoch [16/100], Step [700/782], Loss: 0.6925\n",
      "Epoch [17/100], Step [100/782], Loss: 0.5774\n",
      "Epoch [17/100], Step [200/782], Loss: 0.9783\n",
      "Epoch [17/100], Step [300/782], Loss: 0.6630\n",
      "Epoch [17/100], Step [400/782], Loss: 0.7256\n",
      "Epoch [17/100], Step [500/782], Loss: 0.7685\n",
      "Epoch [17/100], Step [600/782], Loss: 0.7111\n",
      "Epoch [17/100], Step [700/782], Loss: 0.8790\n",
      "Epoch [18/100], Step [100/782], Loss: 0.6174\n",
      "Epoch [18/100], Step [200/782], Loss: 0.7509\n",
      "Epoch [18/100], Step [300/782], Loss: 0.8179\n",
      "Epoch [18/100], Step [400/782], Loss: 0.5431\n",
      "Epoch [18/100], Step [500/782], Loss: 0.6102\n",
      "Epoch [18/100], Step [600/782], Loss: 0.9106\n",
      "Epoch [18/100], Step [700/782], Loss: 0.7306\n",
      "Epoch [19/100], Step [100/782], Loss: 0.6570\n",
      "Epoch [19/100], Step [200/782], Loss: 0.6658\n",
      "Epoch [19/100], Step [300/782], Loss: 0.6812\n",
      "Epoch [19/100], Step [400/782], Loss: 0.7050\n",
      "Epoch [19/100], Step [500/782], Loss: 0.5241\n",
      "Epoch [19/100], Step [600/782], Loss: 0.7561\n",
      "Epoch [19/100], Step [700/782], Loss: 0.7893\n",
      "Epoch [20/100], Step [100/782], Loss: 0.5788\n",
      "Epoch [20/100], Step [200/782], Loss: 0.7439\n",
      "Epoch [20/100], Step [300/782], Loss: 0.6820\n",
      "Epoch [20/100], Step [400/782], Loss: 0.3870\n",
      "Epoch [20/100], Step [500/782], Loss: 0.8762\n",
      "Epoch [20/100], Step [600/782], Loss: 0.6887\n",
      "Epoch [20/100], Step [700/782], Loss: 0.9187\n",
      "Epoch [21/100], Step [100/782], Loss: 0.6467\n",
      "Epoch [21/100], Step [200/782], Loss: 0.7200\n",
      "Epoch [21/100], Step [300/782], Loss: 0.7508\n",
      "Epoch [21/100], Step [400/782], Loss: 0.7453\n",
      "Epoch [21/100], Step [500/782], Loss: 0.6002\n",
      "Epoch [21/100], Step [600/782], Loss: 0.7976\n",
      "Epoch [21/100], Step [700/782], Loss: 1.0220\n",
      "Epoch [22/100], Step [100/782], Loss: 0.6428\n",
      "Epoch [22/100], Step [200/782], Loss: 0.8207\n",
      "Epoch [22/100], Step [300/782], Loss: 0.6540\n",
      "Epoch [22/100], Step [400/782], Loss: 0.6402\n",
      "Epoch [22/100], Step [500/782], Loss: 0.8542\n",
      "Epoch [22/100], Step [600/782], Loss: 0.8238\n",
      "Epoch [22/100], Step [700/782], Loss: 0.7370\n",
      "Epoch [23/100], Step [100/782], Loss: 0.8082\n",
      "Epoch [23/100], Step [200/782], Loss: 0.5412\n",
      "Epoch [23/100], Step [300/782], Loss: 0.5892\n",
      "Epoch [23/100], Step [400/782], Loss: 0.5808\n",
      "Epoch [23/100], Step [500/782], Loss: 0.6002\n",
      "Epoch [23/100], Step [600/782], Loss: 0.6784\n",
      "Epoch [23/100], Step [700/782], Loss: 0.6276\n",
      "Epoch [24/100], Step [100/782], Loss: 0.7268\n",
      "Epoch [24/100], Step [200/782], Loss: 0.7844\n",
      "Epoch [24/100], Step [300/782], Loss: 0.7731\n",
      "Epoch [24/100], Step [400/782], Loss: 0.8101\n",
      "Epoch [24/100], Step [500/782], Loss: 0.6456\n",
      "Epoch [24/100], Step [600/782], Loss: 0.7214\n",
      "Epoch [24/100], Step [700/782], Loss: 0.6912\n",
      "Epoch [25/100], Step [100/782], Loss: 0.5662\n",
      "Epoch [25/100], Step [200/782], Loss: 0.8222\n",
      "Epoch [25/100], Step [300/782], Loss: 0.5861\n",
      "Epoch [25/100], Step [400/782], Loss: 0.7419\n",
      "Epoch [25/100], Step [500/782], Loss: 0.7012\n",
      "Epoch [25/100], Step [600/782], Loss: 0.8191\n",
      "Epoch [25/100], Step [700/782], Loss: 0.6794\n",
      "Epoch [26/100], Step [100/782], Loss: 0.8585\n",
      "Epoch [26/100], Step [200/782], Loss: 0.8973\n",
      "Epoch [26/100], Step [300/782], Loss: 0.4485\n",
      "Epoch [26/100], Step [400/782], Loss: 0.6479\n",
      "Epoch [26/100], Step [500/782], Loss: 0.7548\n",
      "Epoch [26/100], Step [600/782], Loss: 0.6226\n",
      "Epoch [26/100], Step [700/782], Loss: 0.6081\n",
      "Epoch [27/100], Step [100/782], Loss: 0.6797\n",
      "Epoch [27/100], Step [200/782], Loss: 0.6290\n",
      "Epoch [27/100], Step [300/782], Loss: 0.6587\n",
      "Epoch [27/100], Step [400/782], Loss: 0.6464\n",
      "Epoch [27/100], Step [500/782], Loss: 0.7747\n",
      "Epoch [27/100], Step [600/782], Loss: 0.7834\n",
      "Epoch [27/100], Step [700/782], Loss: 0.7201\n",
      "Epoch [28/100], Step [100/782], Loss: 0.6301\n",
      "Epoch [28/100], Step [200/782], Loss: 0.6179\n",
      "Epoch [28/100], Step [300/782], Loss: 0.8839\n",
      "Epoch [28/100], Step [400/782], Loss: 0.8127\n",
      "Epoch [28/100], Step [500/782], Loss: 0.6308\n",
      "Epoch [28/100], Step [600/782], Loss: 0.3961\n",
      "Epoch [28/100], Step [700/782], Loss: 0.7373\n",
      "Epoch [29/100], Step [100/782], Loss: 0.7339\n",
      "Epoch [29/100], Step [200/782], Loss: 0.6314\n",
      "Epoch [29/100], Step [300/782], Loss: 0.6450\n",
      "Epoch [29/100], Step [400/782], Loss: 0.5997\n",
      "Epoch [29/100], Step [500/782], Loss: 0.5186\n",
      "Epoch [29/100], Step [600/782], Loss: 0.9372\n",
      "Epoch [29/100], Step [700/782], Loss: 0.8041\n",
      "Epoch [30/100], Step [100/782], Loss: 0.7216\n",
      "Epoch [30/100], Step [200/782], Loss: 0.7531\n",
      "Epoch [30/100], Step [300/782], Loss: 0.7273\n",
      "Epoch [30/100], Step [400/782], Loss: 1.0005\n",
      "Epoch [30/100], Step [500/782], Loss: 0.6355\n",
      "Epoch [30/100], Step [600/782], Loss: 0.6850\n",
      "Epoch [30/100], Step [700/782], Loss: 0.7882\n",
      "Epoch [31/100], Step [100/782], Loss: 0.7397\n",
      "Epoch [31/100], Step [200/782], Loss: 0.7874\n",
      "Epoch [31/100], Step [300/782], Loss: 0.6906\n",
      "Epoch [31/100], Step [400/782], Loss: 0.6872\n",
      "Epoch [31/100], Step [500/782], Loss: 0.6110\n",
      "Epoch [31/100], Step [600/782], Loss: 0.7973\n",
      "Epoch [31/100], Step [700/782], Loss: 0.7149\n",
      "Epoch [32/100], Step [100/782], Loss: 0.9296\n",
      "Epoch [32/100], Step [200/782], Loss: 0.6635\n",
      "Epoch [32/100], Step [300/782], Loss: 0.7520\n",
      "Epoch [32/100], Step [400/782], Loss: 0.8178\n",
      "Epoch [32/100], Step [500/782], Loss: 0.8447\n",
      "Epoch [32/100], Step [600/782], Loss: 0.6493\n",
      "Epoch [32/100], Step [700/782], Loss: 0.6317\n",
      "Epoch [33/100], Step [100/782], Loss: 1.0104\n",
      "Epoch [33/100], Step [200/782], Loss: 0.7245\n",
      "Epoch [33/100], Step [300/782], Loss: 0.5414\n",
      "Epoch [33/100], Step [400/782], Loss: 0.6480\n",
      "Epoch [33/100], Step [500/782], Loss: 0.7324\n",
      "Epoch [33/100], Step [600/782], Loss: 0.6621\n",
      "Epoch [33/100], Step [700/782], Loss: 0.6647\n",
      "Epoch [34/100], Step [100/782], Loss: 0.8217\n",
      "Epoch [34/100], Step [200/782], Loss: 0.9496\n",
      "Epoch [34/100], Step [300/782], Loss: 0.6686\n",
      "Epoch [34/100], Step [400/782], Loss: 0.7667\n",
      "Epoch [34/100], Step [500/782], Loss: 0.5906\n",
      "Epoch [34/100], Step [600/782], Loss: 0.8597\n",
      "Epoch [34/100], Step [700/782], Loss: 0.5902\n",
      "Epoch [35/100], Step [100/782], Loss: 0.8363\n",
      "Epoch [35/100], Step [200/782], Loss: 0.6627\n",
      "Epoch [35/100], Step [300/782], Loss: 0.9780\n",
      "Epoch [35/100], Step [400/782], Loss: 0.6763\n",
      "Epoch [35/100], Step [500/782], Loss: 0.6249\n",
      "Epoch [35/100], Step [600/782], Loss: 0.6695\n",
      "Epoch [35/100], Step [700/782], Loss: 0.5557\n",
      "Epoch [36/100], Step [100/782], Loss: 0.7523\n",
      "Epoch [36/100], Step [200/782], Loss: 0.5224\n",
      "Epoch [36/100], Step [300/782], Loss: 0.6955\n",
      "Epoch [36/100], Step [400/782], Loss: 0.5853\n",
      "Epoch [36/100], Step [500/782], Loss: 0.7270\n",
      "Epoch [36/100], Step [600/782], Loss: 0.8534\n",
      "Epoch [36/100], Step [700/782], Loss: 0.8012\n",
      "Epoch [37/100], Step [100/782], Loss: 0.5348\n",
      "Epoch [37/100], Step [200/782], Loss: 0.5921\n",
      "Epoch [37/100], Step [300/782], Loss: 0.6342\n",
      "Epoch [37/100], Step [400/782], Loss: 0.7201\n",
      "Epoch [37/100], Step [500/782], Loss: 0.6090\n",
      "Epoch [37/100], Step [600/782], Loss: 0.7557\n",
      "Epoch [37/100], Step [700/782], Loss: 0.6363\n",
      "Epoch [38/100], Step [100/782], Loss: 0.6023\n",
      "Epoch [38/100], Step [200/782], Loss: 0.6247\n",
      "Epoch [38/100], Step [300/782], Loss: 0.7205\n",
      "Epoch [38/100], Step [400/782], Loss: 0.6516\n",
      "Epoch [38/100], Step [500/782], Loss: 0.6508\n",
      "Epoch [38/100], Step [600/782], Loss: 0.6014\n",
      "Epoch [38/100], Step [700/782], Loss: 0.7184\n",
      "Epoch [39/100], Step [100/782], Loss: 0.5838\n",
      "Epoch [39/100], Step [200/782], Loss: 0.9050\n",
      "Epoch [39/100], Step [300/782], Loss: 0.7816\n",
      "Epoch [39/100], Step [400/782], Loss: 0.6280\n",
      "Epoch [39/100], Step [500/782], Loss: 0.8978\n",
      "Epoch [39/100], Step [600/782], Loss: 0.7733\n",
      "Epoch [39/100], Step [700/782], Loss: 0.6808\n",
      "Epoch [40/100], Step [100/782], Loss: 0.6878\n",
      "Epoch [40/100], Step [200/782], Loss: 0.6209\n",
      "Epoch [40/100], Step [300/782], Loss: 0.7579\n",
      "Epoch [40/100], Step [400/782], Loss: 0.5953\n",
      "Epoch [40/100], Step [500/782], Loss: 0.6998\n",
      "Epoch [40/100], Step [600/782], Loss: 0.5485\n",
      "Epoch [40/100], Step [700/782], Loss: 0.7560\n",
      "Epoch [41/100], Step [100/782], Loss: 0.8624\n",
      "Epoch [41/100], Step [200/782], Loss: 0.6839\n",
      "Epoch [41/100], Step [300/782], Loss: 0.7464\n",
      "Epoch [41/100], Step [400/782], Loss: 0.8471\n",
      "Epoch [41/100], Step [500/782], Loss: 0.7898\n",
      "Epoch [41/100], Step [600/782], Loss: 0.6198\n",
      "Epoch [41/100], Step [700/782], Loss: 0.7713\n",
      "Epoch [42/100], Step [100/782], Loss: 0.5291\n",
      "Epoch [42/100], Step [200/782], Loss: 0.7188\n",
      "Epoch [42/100], Step [300/782], Loss: 0.7319\n",
      "Epoch [42/100], Step [400/782], Loss: 0.6948\n",
      "Epoch [42/100], Step [500/782], Loss: 0.8097\n",
      "Epoch [42/100], Step [600/782], Loss: 0.5195\n",
      "Epoch [42/100], Step [700/782], Loss: 0.4359\n",
      "Epoch [43/100], Step [100/782], Loss: 0.6198\n",
      "Epoch [43/100], Step [200/782], Loss: 0.6132\n",
      "Epoch [43/100], Step [300/782], Loss: 0.5496\n",
      "Epoch [43/100], Step [400/782], Loss: 0.5275\n",
      "Epoch [43/100], Step [500/782], Loss: 0.6631\n",
      "Epoch [43/100], Step [600/782], Loss: 0.5292\n",
      "Epoch [43/100], Step [700/782], Loss: 0.7128\n",
      "Epoch [44/100], Step [100/782], Loss: 0.7638\n",
      "Epoch [44/100], Step [200/782], Loss: 0.7714\n",
      "Epoch [44/100], Step [300/782], Loss: 0.6601\n",
      "Epoch [44/100], Step [400/782], Loss: 0.7589\n",
      "Epoch [44/100], Step [500/782], Loss: 0.6952\n",
      "Epoch [44/100], Step [600/782], Loss: 0.6559\n",
      "Epoch [44/100], Step [700/782], Loss: 0.8069\n",
      "Epoch [45/100], Step [100/782], Loss: 0.7784\n",
      "Epoch [45/100], Step [200/782], Loss: 0.7501\n",
      "Epoch [45/100], Step [300/782], Loss: 0.6351\n",
      "Epoch [45/100], Step [400/782], Loss: 0.5427\n",
      "Epoch [45/100], Step [500/782], Loss: 0.8474\n",
      "Epoch [45/100], Step [600/782], Loss: 0.7819\n",
      "Epoch [45/100], Step [700/782], Loss: 0.5453\n",
      "Epoch [46/100], Step [100/782], Loss: 0.5981\n",
      "Epoch [46/100], Step [200/782], Loss: 0.5549\n",
      "Epoch [46/100], Step [300/782], Loss: 0.5439\n",
      "Epoch [46/100], Step [400/782], Loss: 0.5922\n",
      "Epoch [46/100], Step [500/782], Loss: 0.8949\n",
      "Epoch [46/100], Step [600/782], Loss: 0.6951\n",
      "Epoch [46/100], Step [700/782], Loss: 0.4772\n",
      "Epoch [47/100], Step [100/782], Loss: 0.8136\n",
      "Epoch [47/100], Step [200/782], Loss: 0.6699\n",
      "Epoch [47/100], Step [300/782], Loss: 0.9779\n",
      "Epoch [47/100], Step [400/782], Loss: 0.6316\n",
      "Epoch [47/100], Step [500/782], Loss: 0.9225\n",
      "Epoch [47/100], Step [600/782], Loss: 0.6839\n",
      "Epoch [47/100], Step [700/782], Loss: 0.5875\n",
      "Epoch [48/100], Step [100/782], Loss: 0.5970\n",
      "Epoch [48/100], Step [200/782], Loss: 0.4605\n",
      "Epoch [48/100], Step [300/782], Loss: 0.8678\n",
      "Epoch [48/100], Step [400/782], Loss: 0.9416\n",
      "Epoch [48/100], Step [500/782], Loss: 0.7134\n",
      "Epoch [48/100], Step [600/782], Loss: 0.7772\n",
      "Epoch [48/100], Step [700/782], Loss: 0.5826\n",
      "Epoch [49/100], Step [100/782], Loss: 0.5097\n",
      "Epoch [49/100], Step [200/782], Loss: 0.5178\n",
      "Epoch [49/100], Step [300/782], Loss: 0.7850\n",
      "Epoch [49/100], Step [400/782], Loss: 0.6628\n",
      "Epoch [49/100], Step [500/782], Loss: 0.6585\n",
      "Epoch [49/100], Step [600/782], Loss: 0.8162\n",
      "Epoch [49/100], Step [700/782], Loss: 0.6703\n",
      "Epoch [50/100], Step [100/782], Loss: 0.6586\n",
      "Epoch [50/100], Step [200/782], Loss: 0.8128\n",
      "Epoch [50/100], Step [300/782], Loss: 0.6814\n",
      "Epoch [50/100], Step [400/782], Loss: 0.9737\n",
      "Epoch [50/100], Step [500/782], Loss: 0.7482\n",
      "Epoch [50/100], Step [600/782], Loss: 0.5651\n",
      "Epoch [50/100], Step [700/782], Loss: 0.6651\n",
      "Epoch [51/100], Step [100/782], Loss: 0.8002\n",
      "Epoch [51/100], Step [200/782], Loss: 0.7325\n",
      "Epoch [51/100], Step [300/782], Loss: 0.6981\n",
      "Epoch [51/100], Step [400/782], Loss: 0.7196\n",
      "Epoch [51/100], Step [500/782], Loss: 0.5447\n",
      "Epoch [51/100], Step [600/782], Loss: 0.5403\n",
      "Epoch [51/100], Step [700/782], Loss: 0.7805\n",
      "Epoch [52/100], Step [100/782], Loss: 0.5982\n",
      "Epoch [52/100], Step [200/782], Loss: 0.6110\n",
      "Epoch [52/100], Step [300/782], Loss: 0.7679\n",
      "Epoch [52/100], Step [400/782], Loss: 0.7374\n",
      "Epoch [52/100], Step [500/782], Loss: 0.7461\n",
      "Epoch [52/100], Step [600/782], Loss: 0.7129\n",
      "Epoch [52/100], Step [700/782], Loss: 0.6940\n",
      "Epoch [53/100], Step [100/782], Loss: 0.8388\n",
      "Epoch [53/100], Step [200/782], Loss: 0.8599\n",
      "Epoch [53/100], Step [300/782], Loss: 0.7810\n",
      "Epoch [53/100], Step [400/782], Loss: 0.8303\n",
      "Epoch [53/100], Step [500/782], Loss: 0.6715\n",
      "Epoch [53/100], Step [600/782], Loss: 0.7879\n",
      "Epoch [53/100], Step [700/782], Loss: 0.8618\n",
      "Epoch [54/100], Step [100/782], Loss: 0.7146\n",
      "Epoch [54/100], Step [200/782], Loss: 0.5342\n",
      "Epoch [54/100], Step [300/782], Loss: 0.5933\n",
      "Epoch [54/100], Step [400/782], Loss: 0.6434\n",
      "Epoch [54/100], Step [500/782], Loss: 0.7525\n",
      "Epoch [54/100], Step [600/782], Loss: 0.7254\n",
      "Epoch [54/100], Step [700/782], Loss: 0.6469\n",
      "Epoch [55/100], Step [100/782], Loss: 0.7596\n",
      "Epoch [55/100], Step [200/782], Loss: 0.6346\n",
      "Epoch [55/100], Step [300/782], Loss: 0.5286\n",
      "Epoch [55/100], Step [400/782], Loss: 0.4646\n",
      "Epoch [55/100], Step [500/782], Loss: 0.7893\n",
      "Epoch [55/100], Step [600/782], Loss: 0.6061\n",
      "Epoch [55/100], Step [700/782], Loss: 0.6218\n",
      "Epoch [56/100], Step [100/782], Loss: 0.5808\n",
      "Epoch [56/100], Step [200/782], Loss: 0.7875\n",
      "Epoch [56/100], Step [300/782], Loss: 0.7584\n",
      "Epoch [56/100], Step [400/782], Loss: 0.6002\n",
      "Epoch [56/100], Step [500/782], Loss: 0.5200\n",
      "Epoch [56/100], Step [600/782], Loss: 0.7815\n",
      "Epoch [56/100], Step [700/782], Loss: 0.6327\n",
      "Epoch [57/100], Step [100/782], Loss: 0.6097\n",
      "Epoch [57/100], Step [200/782], Loss: 0.6594\n",
      "Epoch [57/100], Step [300/782], Loss: 0.5616\n",
      "Epoch [57/100], Step [400/782], Loss: 0.5087\n",
      "Epoch [57/100], Step [500/782], Loss: 0.7336\n",
      "Epoch [57/100], Step [600/782], Loss: 0.5514\n",
      "Epoch [57/100], Step [700/782], Loss: 0.7322\n",
      "Epoch [58/100], Step [100/782], Loss: 0.7248\n",
      "Epoch [58/100], Step [200/782], Loss: 0.7301\n",
      "Epoch [58/100], Step [300/782], Loss: 0.6522\n",
      "Epoch [58/100], Step [400/782], Loss: 0.6865\n",
      "Epoch [58/100], Step [500/782], Loss: 0.9371\n",
      "Epoch [58/100], Step [600/782], Loss: 0.6808\n",
      "Epoch [58/100], Step [700/782], Loss: 0.7382\n",
      "Epoch [59/100], Step [100/782], Loss: 0.7629\n",
      "Epoch [59/100], Step [200/782], Loss: 0.6137\n",
      "Epoch [59/100], Step [300/782], Loss: 0.6063\n",
      "Epoch [59/100], Step [400/782], Loss: 0.4453\n",
      "Epoch [59/100], Step [500/782], Loss: 0.7861\n",
      "Epoch [59/100], Step [600/782], Loss: 0.5280\n",
      "Epoch [59/100], Step [700/782], Loss: 0.6736\n",
      "Epoch [60/100], Step [100/782], Loss: 0.4538\n",
      "Epoch [60/100], Step [200/782], Loss: 0.5059\n",
      "Epoch [60/100], Step [300/782], Loss: 0.6484\n",
      "Epoch [60/100], Step [400/782], Loss: 0.4791\n",
      "Epoch [60/100], Step [500/782], Loss: 0.6212\n",
      "Epoch [60/100], Step [600/782], Loss: 0.8908\n",
      "Epoch [60/100], Step [700/782], Loss: 0.7653\n",
      "Epoch [61/100], Step [100/782], Loss: 0.6174\n",
      "Epoch [61/100], Step [200/782], Loss: 0.7115\n",
      "Epoch [61/100], Step [300/782], Loss: 0.9935\n",
      "Epoch [61/100], Step [400/782], Loss: 0.5105\n",
      "Epoch [61/100], Step [500/782], Loss: 0.5201\n",
      "Epoch [61/100], Step [600/782], Loss: 0.6177\n",
      "Epoch [61/100], Step [700/782], Loss: 0.6812\n",
      "Epoch [62/100], Step [100/782], Loss: 0.7063\n",
      "Epoch [62/100], Step [200/782], Loss: 0.8583\n",
      "Epoch [62/100], Step [300/782], Loss: 0.6031\n",
      "Epoch [62/100], Step [400/782], Loss: 0.7304\n",
      "Epoch [62/100], Step [500/782], Loss: 0.7782\n",
      "Epoch [62/100], Step [600/782], Loss: 0.6374\n",
      "Epoch [62/100], Step [700/782], Loss: 0.7471\n",
      "Epoch [63/100], Step [100/782], Loss: 0.5081\n",
      "Epoch [63/100], Step [200/782], Loss: 0.8500\n",
      "Epoch [63/100], Step [300/782], Loss: 0.5751\n",
      "Epoch [63/100], Step [400/782], Loss: 0.9753\n",
      "Epoch [63/100], Step [500/782], Loss: 0.4967\n",
      "Epoch [63/100], Step [600/782], Loss: 0.9156\n",
      "Epoch [63/100], Step [700/782], Loss: 0.7887\n",
      "Epoch [64/100], Step [100/782], Loss: 0.5703\n",
      "Epoch [64/100], Step [200/782], Loss: 0.6977\n",
      "Epoch [64/100], Step [300/782], Loss: 0.7946\n",
      "Epoch [64/100], Step [400/782], Loss: 0.7009\n",
      "Epoch [64/100], Step [500/782], Loss: 0.7128\n",
      "Epoch [64/100], Step [600/782], Loss: 0.5997\n",
      "Epoch [64/100], Step [700/782], Loss: 0.8016\n",
      "Epoch [65/100], Step [100/782], Loss: 0.6239\n",
      "Epoch [65/100], Step [200/782], Loss: 0.5178\n",
      "Epoch [65/100], Step [300/782], Loss: 0.5647\n",
      "Epoch [65/100], Step [400/782], Loss: 0.5971\n",
      "Epoch [65/100], Step [500/782], Loss: 0.5184\n",
      "Epoch [65/100], Step [600/782], Loss: 0.6280\n",
      "Epoch [65/100], Step [700/782], Loss: 0.7877\n",
      "Epoch [66/100], Step [100/782], Loss: 0.7614\n",
      "Epoch [66/100], Step [200/782], Loss: 0.5468\n",
      "Epoch [66/100], Step [300/782], Loss: 0.4451\n",
      "Epoch [66/100], Step [400/782], Loss: 0.8108\n",
      "Epoch [66/100], Step [500/782], Loss: 0.5768\n",
      "Epoch [66/100], Step [600/782], Loss: 0.6571\n",
      "Epoch [66/100], Step [700/782], Loss: 0.6447\n",
      "Epoch [67/100], Step [100/782], Loss: 0.6080\n",
      "Epoch [67/100], Step [200/782], Loss: 0.5299\n",
      "Epoch [67/100], Step [300/782], Loss: 0.7193\n",
      "Epoch [67/100], Step [400/782], Loss: 0.7268\n",
      "Epoch [67/100], Step [500/782], Loss: 0.4594\n",
      "Epoch [67/100], Step [600/782], Loss: 0.6885\n",
      "Epoch [67/100], Step [700/782], Loss: 0.7571\n",
      "Epoch [68/100], Step [100/782], Loss: 0.5218\n",
      "Epoch [68/100], Step [200/782], Loss: 0.6341\n",
      "Epoch [68/100], Step [300/782], Loss: 0.5510\n",
      "Epoch [68/100], Step [400/782], Loss: 0.6610\n",
      "Epoch [68/100], Step [500/782], Loss: 0.6588\n",
      "Epoch [68/100], Step [600/782], Loss: 0.6373\n",
      "Epoch [68/100], Step [700/782], Loss: 0.6422\n",
      "Epoch [69/100], Step [100/782], Loss: 0.7623\n",
      "Epoch [69/100], Step [200/782], Loss: 0.6366\n",
      "Epoch [69/100], Step [300/782], Loss: 0.7590\n",
      "Epoch [69/100], Step [400/782], Loss: 0.6655\n",
      "Epoch [69/100], Step [500/782], Loss: 0.4684\n",
      "Epoch [69/100], Step [600/782], Loss: 0.5007\n",
      "Epoch [69/100], Step [700/782], Loss: 0.8668\n",
      "Epoch [70/100], Step [100/782], Loss: 0.8790\n",
      "Epoch [70/100], Step [200/782], Loss: 0.6010\n",
      "Epoch [70/100], Step [300/782], Loss: 0.7793\n",
      "Epoch [70/100], Step [400/782], Loss: 0.7826\n",
      "Epoch [70/100], Step [500/782], Loss: 0.9122\n",
      "Epoch [70/100], Step [600/782], Loss: 0.6939\n",
      "Epoch [70/100], Step [700/782], Loss: 0.6332\n",
      "Epoch [71/100], Step [100/782], Loss: 0.6544\n",
      "Epoch [71/100], Step [200/782], Loss: 0.7257\n",
      "Epoch [71/100], Step [300/782], Loss: 0.5254\n",
      "Epoch [71/100], Step [400/782], Loss: 0.5246\n",
      "Epoch [71/100], Step [500/782], Loss: 0.4195\n",
      "Epoch [71/100], Step [600/782], Loss: 0.6916\n",
      "Epoch [71/100], Step [700/782], Loss: 0.7273\n",
      "Epoch [72/100], Step [100/782], Loss: 0.5332\n",
      "Epoch [72/100], Step [200/782], Loss: 0.9022\n",
      "Epoch [72/100], Step [300/782], Loss: 0.6739\n",
      "Epoch [72/100], Step [400/782], Loss: 0.7886\n",
      "Epoch [72/100], Step [500/782], Loss: 0.6878\n",
      "Epoch [72/100], Step [600/782], Loss: 0.8681\n",
      "Epoch [72/100], Step [700/782], Loss: 0.6693\n",
      "Epoch [73/100], Step [100/782], Loss: 0.7369\n",
      "Epoch [73/100], Step [200/782], Loss: 0.5926\n",
      "Epoch [73/100], Step [300/782], Loss: 0.8365\n",
      "Epoch [73/100], Step [400/782], Loss: 0.6850\n",
      "Epoch [73/100], Step [500/782], Loss: 0.5959\n",
      "Epoch [73/100], Step [600/782], Loss: 0.6616\n",
      "Epoch [73/100], Step [700/782], Loss: 0.6409\n",
      "Epoch [74/100], Step [100/782], Loss: 0.6133\n",
      "Epoch [74/100], Step [200/782], Loss: 0.5947\n",
      "Epoch [74/100], Step [300/782], Loss: 0.4983\n",
      "Epoch [74/100], Step [400/782], Loss: 0.7389\n",
      "Epoch [74/100], Step [500/782], Loss: 0.7144\n",
      "Epoch [74/100], Step [600/782], Loss: 0.4918\n",
      "Epoch [74/100], Step [700/782], Loss: 0.6280\n",
      "Epoch [75/100], Step [100/782], Loss: 0.4732\n",
      "Epoch [75/100], Step [200/782], Loss: 0.6381\n",
      "Epoch [75/100], Step [300/782], Loss: 0.5559\n",
      "Epoch [75/100], Step [400/782], Loss: 0.5289\n",
      "Epoch [75/100], Step [500/782], Loss: 0.6764\n",
      "Epoch [75/100], Step [600/782], Loss: 0.6512\n",
      "Epoch [75/100], Step [700/782], Loss: 0.5740\n",
      "Epoch [76/100], Step [100/782], Loss: 0.6987\n",
      "Epoch [76/100], Step [200/782], Loss: 0.3859\n",
      "Epoch [76/100], Step [300/782], Loss: 0.6115\n",
      "Epoch [76/100], Step [400/782], Loss: 0.6583\n",
      "Epoch [76/100], Step [500/782], Loss: 0.8896\n",
      "Epoch [76/100], Step [600/782], Loss: 0.8272\n",
      "Epoch [76/100], Step [700/782], Loss: 0.6173\n",
      "Epoch [77/100], Step [100/782], Loss: 0.5532\n",
      "Epoch [77/100], Step [200/782], Loss: 0.6093\n",
      "Epoch [77/100], Step [300/782], Loss: 0.5124\n",
      "Epoch [77/100], Step [400/782], Loss: 0.8285\n",
      "Epoch [77/100], Step [500/782], Loss: 0.8566\n",
      "Epoch [77/100], Step [600/782], Loss: 0.5787\n",
      "Epoch [77/100], Step [700/782], Loss: 0.4632\n",
      "Epoch [78/100], Step [100/782], Loss: 0.8462\n",
      "Epoch [78/100], Step [200/782], Loss: 0.7328\n",
      "Epoch [78/100], Step [300/782], Loss: 0.8211\n",
      "Epoch [78/100], Step [400/782], Loss: 0.6712\n",
      "Epoch [78/100], Step [500/782], Loss: 0.7419\n",
      "Epoch [78/100], Step [600/782], Loss: 0.6560\n",
      "Epoch [78/100], Step [700/782], Loss: 0.7381\n",
      "Epoch [79/100], Step [100/782], Loss: 0.7446\n",
      "Epoch [79/100], Step [200/782], Loss: 0.5920\n",
      "Epoch [79/100], Step [300/782], Loss: 0.5942\n",
      "Epoch [79/100], Step [400/782], Loss: 0.9005\n",
      "Epoch [79/100], Step [500/782], Loss: 0.7685\n",
      "Epoch [79/100], Step [600/782], Loss: 0.6471\n",
      "Epoch [79/100], Step [700/782], Loss: 0.6521\n",
      "Epoch [80/100], Step [100/782], Loss: 0.7615\n",
      "Epoch [80/100], Step [200/782], Loss: 0.6150\n",
      "Epoch [80/100], Step [300/782], Loss: 0.6119\n",
      "Epoch [80/100], Step [400/782], Loss: 0.6786\n",
      "Epoch [80/100], Step [500/782], Loss: 0.4427\n",
      "Epoch [80/100], Step [600/782], Loss: 0.5628\n",
      "Epoch [80/100], Step [700/782], Loss: 0.7376\n",
      "Epoch [81/100], Step [100/782], Loss: 0.5389\n",
      "Epoch [81/100], Step [200/782], Loss: 0.6187\n",
      "Epoch [81/100], Step [300/782], Loss: 0.8163\n",
      "Epoch [81/100], Step [400/782], Loss: 0.4648\n",
      "Epoch [81/100], Step [500/782], Loss: 0.7858\n",
      "Epoch [81/100], Step [600/782], Loss: 0.6593\n",
      "Epoch [81/100], Step [700/782], Loss: 0.8053\n",
      "Epoch [82/100], Step [100/782], Loss: 0.5959\n",
      "Epoch [82/100], Step [200/782], Loss: 0.7460\n",
      "Epoch [82/100], Step [300/782], Loss: 0.7810\n",
      "Epoch [82/100], Step [400/782], Loss: 0.8607\n",
      "Epoch [82/100], Step [500/782], Loss: 0.6502\n",
      "Epoch [82/100], Step [600/782], Loss: 0.5859\n",
      "Epoch [82/100], Step [700/782], Loss: 0.5828\n",
      "Epoch [83/100], Step [100/782], Loss: 0.5525\n",
      "Epoch [83/100], Step [200/782], Loss: 0.7138\n",
      "Epoch [83/100], Step [300/782], Loss: 0.4692\n",
      "Epoch [83/100], Step [400/782], Loss: 0.5964\n",
      "Epoch [83/100], Step [500/782], Loss: 0.5347\n",
      "Epoch [83/100], Step [600/782], Loss: 0.8008\n",
      "Epoch [83/100], Step [700/782], Loss: 0.6739\n",
      "Epoch [84/100], Step [100/782], Loss: 0.7159\n",
      "Epoch [84/100], Step [200/782], Loss: 0.7615\n",
      "Epoch [84/100], Step [300/782], Loss: 0.5924\n",
      "Epoch [84/100], Step [400/782], Loss: 0.5796\n",
      "Epoch [84/100], Step [500/782], Loss: 0.7654\n",
      "Epoch [84/100], Step [600/782], Loss: 0.7748\n",
      "Epoch [84/100], Step [700/782], Loss: 0.6170\n",
      "Epoch [85/100], Step [100/782], Loss: 0.7176\n",
      "Epoch [85/100], Step [200/782], Loss: 0.6833\n",
      "Epoch [85/100], Step [300/782], Loss: 0.7439\n",
      "Epoch [85/100], Step [400/782], Loss: 0.7260\n",
      "Epoch [85/100], Step [500/782], Loss: 0.8211\n",
      "Epoch [85/100], Step [600/782], Loss: 0.8631\n",
      "Epoch [85/100], Step [700/782], Loss: 0.6111\n",
      "Epoch [86/100], Step [100/782], Loss: 0.6078\n",
      "Epoch [86/100], Step [200/782], Loss: 0.5902\n",
      "Epoch [86/100], Step [300/782], Loss: 0.5138\n",
      "Epoch [86/100], Step [400/782], Loss: 0.5140\n",
      "Epoch [86/100], Step [500/782], Loss: 0.6015\n",
      "Epoch [86/100], Step [600/782], Loss: 0.7531\n",
      "Epoch [86/100], Step [700/782], Loss: 0.6518\n",
      "Epoch [87/100], Step [100/782], Loss: 0.7211\n",
      "Epoch [87/100], Step [200/782], Loss: 0.6084\n",
      "Epoch [87/100], Step [300/782], Loss: 0.9635\n",
      "Epoch [87/100], Step [400/782], Loss: 0.6445\n",
      "Epoch [87/100], Step [500/782], Loss: 0.8039\n",
      "Epoch [87/100], Step [600/782], Loss: 0.7605\n",
      "Epoch [87/100], Step [700/782], Loss: 0.6058\n",
      "Epoch [88/100], Step [100/782], Loss: 0.7547\n",
      "Epoch [88/100], Step [200/782], Loss: 0.7143\n",
      "Epoch [88/100], Step [300/782], Loss: 0.7254\n",
      "Epoch [88/100], Step [400/782], Loss: 0.5630\n",
      "Epoch [88/100], Step [500/782], Loss: 0.6646\n",
      "Epoch [88/100], Step [600/782], Loss: 0.6555\n",
      "Epoch [88/100], Step [700/782], Loss: 0.8808\n",
      "Epoch [89/100], Step [100/782], Loss: 0.7886\n",
      "Epoch [89/100], Step [200/782], Loss: 0.8396\n",
      "Epoch [89/100], Step [300/782], Loss: 0.4566\n",
      "Epoch [89/100], Step [400/782], Loss: 0.7518\n",
      "Epoch [89/100], Step [500/782], Loss: 0.7255\n",
      "Epoch [89/100], Step [600/782], Loss: 0.6899\n",
      "Epoch [89/100], Step [700/782], Loss: 0.8338\n",
      "Epoch [90/100], Step [100/782], Loss: 0.5846\n",
      "Epoch [90/100], Step [200/782], Loss: 0.5606\n",
      "Epoch [90/100], Step [300/782], Loss: 0.9518\n",
      "Epoch [90/100], Step [400/782], Loss: 0.7362\n",
      "Epoch [90/100], Step [500/782], Loss: 0.6753\n",
      "Epoch [90/100], Step [600/782], Loss: 0.6203\n",
      "Epoch [90/100], Step [700/782], Loss: 0.9134\n",
      "Epoch [91/100], Step [100/782], Loss: 0.6348\n",
      "Epoch [91/100], Step [200/782], Loss: 0.6748\n",
      "Epoch [91/100], Step [300/782], Loss: 0.6487\n",
      "Epoch [91/100], Step [400/782], Loss: 0.9350\n",
      "Epoch [91/100], Step [500/782], Loss: 0.8651\n",
      "Epoch [91/100], Step [600/782], Loss: 0.7020\n",
      "Epoch [91/100], Step [700/782], Loss: 0.6102\n",
      "Epoch [92/100], Step [100/782], Loss: 0.6615\n",
      "Epoch [92/100], Step [200/782], Loss: 0.5860\n",
      "Epoch [92/100], Step [300/782], Loss: 0.6759\n",
      "Epoch [92/100], Step [400/782], Loss: 0.4601\n",
      "Epoch [92/100], Step [500/782], Loss: 0.6747\n",
      "Epoch [92/100], Step [600/782], Loss: 0.4767\n",
      "Epoch [92/100], Step [700/782], Loss: 1.0071\n",
      "Epoch [93/100], Step [100/782], Loss: 0.7373\n",
      "Epoch [93/100], Step [200/782], Loss: 0.8844\n",
      "Epoch [93/100], Step [300/782], Loss: 0.8642\n",
      "Epoch [93/100], Step [400/782], Loss: 0.7499\n",
      "Epoch [93/100], Step [500/782], Loss: 0.5850\n",
      "Epoch [93/100], Step [600/782], Loss: 0.6006\n",
      "Epoch [93/100], Step [700/782], Loss: 0.7103\n",
      "Epoch [94/100], Step [100/782], Loss: 0.7391\n",
      "Epoch [94/100], Step [200/782], Loss: 0.7005\n",
      "Epoch [94/100], Step [300/782], Loss: 0.4771\n",
      "Epoch [94/100], Step [400/782], Loss: 0.5669\n",
      "Epoch [94/100], Step [500/782], Loss: 1.0971\n",
      "Epoch [94/100], Step [600/782], Loss: 0.5993\n",
      "Epoch [94/100], Step [700/782], Loss: 0.7744\n",
      "Epoch [95/100], Step [100/782], Loss: 0.8813\n",
      "Epoch [95/100], Step [200/782], Loss: 0.6949\n",
      "Epoch [95/100], Step [300/782], Loss: 0.7090\n",
      "Epoch [95/100], Step [400/782], Loss: 0.6860\n",
      "Epoch [95/100], Step [500/782], Loss: 0.6912\n",
      "Epoch [95/100], Step [600/782], Loss: 0.5147\n",
      "Epoch [95/100], Step [700/782], Loss: 0.6380\n",
      "Epoch [96/100], Step [100/782], Loss: 0.7942\n",
      "Epoch [96/100], Step [200/782], Loss: 0.5552\n",
      "Epoch [96/100], Step [300/782], Loss: 0.7891\n",
      "Epoch [96/100], Step [400/782], Loss: 0.7404\n",
      "Epoch [96/100], Step [500/782], Loss: 0.7508\n",
      "Epoch [96/100], Step [600/782], Loss: 0.5721\n",
      "Epoch [96/100], Step [700/782], Loss: 0.6192\n",
      "Epoch [97/100], Step [100/782], Loss: 0.4631\n",
      "Epoch [97/100], Step [200/782], Loss: 0.7615\n",
      "Epoch [97/100], Step [300/782], Loss: 0.6935\n",
      "Epoch [97/100], Step [400/782], Loss: 0.6953\n",
      "Epoch [97/100], Step [500/782], Loss: 0.6140\n",
      "Epoch [97/100], Step [600/782], Loss: 0.7212\n",
      "Epoch [97/100], Step [700/782], Loss: 0.7630\n",
      "Epoch [98/100], Step [100/782], Loss: 0.5644\n",
      "Epoch [98/100], Step [200/782], Loss: 0.6990\n",
      "Epoch [98/100], Step [300/782], Loss: 0.8157\n",
      "Epoch [98/100], Step [400/782], Loss: 0.5613\n",
      "Epoch [98/100], Step [500/782], Loss: 0.7205\n",
      "Epoch [98/100], Step [600/782], Loss: 0.6162\n",
      "Epoch [98/100], Step [700/782], Loss: 0.7545\n",
      "Epoch [99/100], Step [100/782], Loss: 0.7957\n",
      "Epoch [99/100], Step [200/782], Loss: 0.5297\n",
      "Epoch [99/100], Step [300/782], Loss: 0.6302\n",
      "Epoch [99/100], Step [400/782], Loss: 0.5602\n",
      "Epoch [99/100], Step [500/782], Loss: 0.7909\n",
      "Epoch [99/100], Step [600/782], Loss: 0.8265\n",
      "Epoch [99/100], Step [700/782], Loss: 0.6460\n",
      "Epoch [100/100], Step [100/782], Loss: 0.7618\n",
      "Epoch [100/100], Step [200/782], Loss: 0.7106\n",
      "Epoch [100/100], Step [300/782], Loss: 0.7013\n",
      "Epoch [100/100], Step [400/782], Loss: 0.6239\n",
      "Epoch [100/100], Step [500/782], Loss: 0.6513\n",
      "Epoch [100/100], Step [600/782], Loss: 0.8748\n",
      "Epoch [100/100], Step [700/782], Loss: 0.7796\n",
      "Test Accuracy of the model on the 10000 test images: 78.04 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# CIFAR-10 dataset with data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.1)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
